{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:5em;\">Neural network from scratch</span>\n",
    "\n",
    "**Michał M.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I implement basic neural network using numpy and pandas. I do not use any well known frameworks as Tensorflow or Pytorch, it is just an exercise. I will publish it on Kaggle as a notebook for educational purposes.\n",
    "\n",
    "The idea of this notebook is based on Jan Chorowski Neural Networks class conducted at Institute of Computer Science, University of Wrocław.\n",
    "\n",
    "I will write the code first and then I would add some theory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gradient Descent algorithm finds the minimum of a given function by taking small steps along the function's gradient. In pseudocode:\n",
    "\n",
    ">$\\Theta \\leftarrow \\Theta_0$\n",
    ">\n",
    ">**while** stop condition not met **do**\n",
    ">\n",
    ">$~~~~$$\\Theta \\leftarrow \\Theta - \\alpha \\nabla_\\Theta f(\\Theta)$\n",
    ">\n",
    ">**end while**\n",
    "\n",
    "where $f$ is the function to minimize, $\\nabla_\\Theta f(\\Theta)$ denotes $f$'s gradient at $\\Theta$ and $\\alpha$ is the step size, taking typically values from $10^{-4},\\ldots,10^{-1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GD optimizer\n",
    "\n",
    "def GD(f, Theta0, alpha, stop_tolerance=1e-10, max_steps=1000000, verbose=False):\n",
    "    \"\"\"    \n",
    "    Runs gradient descent algorithm on f.\n",
    "    \n",
    "    The basic iteration is:\n",
    "    val, dVdTheta <- f(Theta)\n",
    "    Theta <- -alpha * dVdTheta\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    f : function that when evalueted on a Theta of same dtype and shape as Theta0\n",
    "        returns a tuple (value, dValdTheta) with dValdTheta of the same shape as Theta\n",
    "        \n",
    "    Theta0 : starting point\n",
    "        \n",
    "    alpha : step length\n",
    "    \n",
    "    stop_tolerance : stop iterations when improvement is below this threhsold\n",
    "        \n",
    "    max_steps : maximum number of steps\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    Theta, history : tuple\n",
    "        - Theta : optimal value of Theta\n",
    "        - history : list of length num_steps containing tuples (Theta, (val, dValdTheta))\n",
    "    \"\"\"\n",
    "    history = []\n",
    "    \n",
    "    Theta = Theta0\n",
    "    value = np.inf\n",
    "    \n",
    "    step = 0\n",
    "    while step < max_steps:\n",
    "        previous_value = value\n",
    "        value, gradient = f(Theta)\n",
    "        history.append([Theta, (value, gradient)])\n",
    "        \n",
    "        if np.abs(previous_value-value) < stop_tolerance:\n",
    "            break\n",
    "        \n",
    "        Theta = Theta - alpha*gradient\n",
    "        \n",
    "        step += 1\n",
    "    \n",
    "    history.append([Theta, f(Theta)])\n",
    "    \n",
    "    if verbose:\n",
    "        print (\"Found optimum at %s in %d steps.\" % (Theta, len(history)))\n",
    "    \n",
    "    return Theta, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rosenbrock function\n",
    "\n",
    "def rosenbrock_v(x, a=1, b=100):\n",
    "    \"\"\"  \n",
    "    Returns the value of Rosenbrock's function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : arguments in a form of list with length 2\n",
    "    \n",
    "    a, b : parameters of Rosenbrock's function\n",
    "            \n",
    "    Returns\n",
    "    ----------\n",
    "    foo : float, value of Rosenbrock's function at x\n",
    "    \"\"\"\n",
    "    return (a - x[0])**2 + b*(x[1] - x[0]**2)**2\n",
    "\n",
    "def rosenbrock(x):\n",
    "    \"\"\"\n",
    "    Returns the value of Rosenbrock's function and its gradient.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    v : argument\n",
    "    \n",
    "    a, b : parameters of rose\n",
    "            \n",
    "    Returns\n",
    "    ----------\n",
    "    foo : ndarray of shape (2,)\n",
    "        - value : value of Rosenbrock's function\n",
    "        - dVdX : ndarray of shape (2,) containing gradient values \n",
    "    \"\"\"\n",
    "    a = 1\n",
    "    b = 100\n",
    "    value  = rosenbrock_v(x)\n",
    "    dVdx0 = -2*(a - x[0]) - 2*b*(x[1]-x[0]**2) * 2*x[0]\n",
    "    dVdx1 = 2*b * (x[1]-x[0]**2)\n",
    "    dVdX = np.array([dVdx0, dVdx1])    \n",
    "    return np.array([value, dVdX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found optimum at [0.99975021 0.99949949] in 9243 steps.\n"
     ]
    }
   ],
   "source": [
    "Xzero = [-0.45, 2.25]\n",
    "Xopt, Xhist = GD(rosenbrock, Xzero, alpha=2e-3, stop_tolerance=1e-10, max_steps=1e6, verbose=True)\n",
    "\n",
    "values_hist = [t[1][0] for t in Xhist]\n",
    "\n",
    "\n",
    "x = np.arange(-2.25,2.25,0.01)\n",
    "y = np.arange(-1,2.5,0.01)\n",
    "X,Y = np.meshgrid(x,y)\n",
    "Z = rosenbrock_v([X, Y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(20, 5))\n",
    "\n",
    "axes[0].grid()\n",
    "axes[0].set_yscale(\"log\")\n",
    "axes[0].plot(values_hist)\n",
    "\n",
    "axes[1].plot([t[0][0] for t in Xhist], [t[0][1] for t in Xhist], color='red')\n",
    "im = axes[1].contour(X, Y, Z, 100, cmap='Blues')\n",
    "fig.colorbar(im, ax=axes[1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
